from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Custom Dataset for Multi-Class Classification
class MultiClassDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]

        encoding = self.tokenizer.encode_plus(
            text,
            max_length=self.max_len,
            truncation=True,
            add_special_tokens=True,
            padding="max_length",
            return_attention_mask=True,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "label": torch.tensor(label, dtype=torch.long),
        }

# Model with Custom Attention Mechanism
class BERTWithAttention(nn.Module):
    def __init__(self, bert_model_name, num_labels):
        super(BERTWithAttention, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name, output_attentions=True)
        self.attention = nn.Linear(self.bert.config.hidden_size, 1)
        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_labels)
        self.dropout = nn.Dropout(0.3)

    def forward(self, input_ids, attention_mask):
        # BERT outputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)
        cls_output = outputs.pooler_output  # Shape: (batch_size, hidden_dim)

        # Custom attention mechanism
        attention_scores = self.attention(hidden_states)  # Shape: (batch_size, seq_len, 1)
        attention_weights = torch.softmax(attention_scores, dim=1)  # Normalize scores
        context_vector = torch.sum(attention_weights * hidden_states, dim=1)  # Weighted sum

        # Concatenate CLS token and context vector
        combined_features = torch.cat([cls_output, context_vector], dim=1)
        combined_features = self.dropout(combined_features)

        # Classification
        logits = self.classifier(combined_features)
        return logits

# Parameters
PRETRAINED_MODEL_NAME = "bert-base-uncased"
MAX_LEN = 128
BATCH_SIZE = 16
EPOCHS = 5
LEARNING_RATE = 2e-5
NUM_CLASSES = 4  # Number of groups

# Example data
import pandas as pd
content = pd.read_csv('Disney.csv', sep=',')
print(content.shape)
texts = [
    "The study focused on the impact of age and gender on climate policies.",
    "Environmental conservation is critical for all age groups.",
    "Sexual health awareness campaigns are important for younger age groups.",
    "Climate change mitigation policies need funding.",
    "Gender roles in climate policies",
]
labels = [2, 0, 1, 0, 2]  # Example labels

# Tokenizer
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)

# Dataset and DataLoader
dataset = MultiClassDataset(texts, labels, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Initialize Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTWithAttention(PRETRAINED_MODEL_NAME, num_labels=NUM_CLASSES)
model = model.to(device)

# Optimizer and Loss
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss()
loss_set = []
# Training Loop
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad() # reset the gradients to zero before the backward pass.
        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loss = total_loss/len(dataloader)
        loss_set.append(loss)

    print(f"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(dataloader)}")

# Inference Example
model.eval()
test_texts = ["Gender roles in our basketball matches", "Environmental awareness is increasing."]
test_encodings = tokenizer(test_texts, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt")
test_input_ids = test_encodings["input_ids"].to(device)
test_attention_mask = test_encodings["attention_mask"].to(device)

with torch.no_grad():
    predictions = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
    predicted_classes = torch.argmax(predictions, dim=1)
    print("Predicted Classes:", predicted_classes.cpu().numpy())

import matplotlib.pyplot as plt
plt.plot(loss_set)
plt.show()