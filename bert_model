from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Custom Dataset for Multi-Class Classification
class MultiClassDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]

        encoding = self.tokenizer.encode_plus(
            text,
            max_length=self.max_len,
            truncation=True,
            add_special_tokens=True,
            padding="max_length",
            return_attention_mask=True,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "label": torch.tensor(label, dtype=torch.long),
        }

# Model with Custom Attention Mechanism
class BERTWithAttention(nn.Module):
    def __init__(self, bert_model_name, num_labels):
        super(BERTWithAttention, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name, output_attentions=True)
        nums_head = 2
        self.attention = nn.Linear(self.bert.config.hidden_size, nums_head)
# The classifier is a linear layer that takes the combined features from the BERT model (the CLS token output and the context vector) and maps them to the output classes for the classification task. 
        self.classifier = nn.Linear(self.bert.config.hidden_size * (1+ nums_head), num_labels)
        self.dropout = nn.Dropout(0.2)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)
        cls_output = outputs.pooler_output  # Shape: (batch_size, hidden_dim)

        # Custom attention mechanism
        attention_scores = self.attention(hidden_states)  # Shape: (batch_size, seq_len, head_num)
        attention_weights = torch.softmax(attention_scores, dim=1)  # Normalize scores
        attention_weights = attention_weights.unsqueeze(-1)  
# New shape: (batch_size, head_num, seq_len, 1)

# Adjust hidden_states shape for compatibility
        hidden_states = hidden_states.unsqueeze(2) 
# New shape: (batch_size, 1, seq_len, feature_dim)

# Compute weighted sum
        context_vector = torch.sum(attention_weights * hidden_states, dim=1)

# Final shape: (batch_size, head_num, feature_dim)
        # Concatenate CLS token and context vector
        combined_features = torch.cat([cls_output.unsqueeze(1), context_vector], dim=1)
        combined_features = self.dropout(combined_features)
        logits = self.classifier(combined_features.view(combined_features.size(0), -1)) # flatten for classifier
        return logits

# Parameters
PRETRAINED_MODEL_NAME = "bert-base-uncased"
MAX_LEN = 128
BATCH_SIZE = 16
EPOCHS = 5
LEARNING_RATE = 2e-7
NUM_CLASSES = 6  # Number of groups

# Example data
import pandas as pd
content = pd.read_csv('Disney.csv', sep=',')
content = content.iloc[:,:-1]
content['transformed_label'] = content['打标类型'].map({
    '品牌热点':0,
    '景色场景': 1,
    '明星热点': 2,
    '生活时事': 3,
    '影视热点': 4,
    '赛事热点': 5
})
content.rename(columns={'打标类型':'type'}, inplace=True)
# 0.8 train and 0.2 test
brands = content.loc[content['transformed_label']==0,'transformed_label'].count()/content.shape[0]
views = content.loc[content["transformed_label"]==1, 'transformed_label'].count()/content.shape[0]
"""
texts = [
    "The study focused on the impact of age and gender on climate policies.",
    "Environmental conservation is critical for all age groups.",
    "Sexual health awareness campaigns are important for younger age groups.",
    "Climate change mitigation policies need funding.",
    "Gender roles in climate policies",
]
labels = [2, 0, 1, 0, 2]  # Example labels
"""
# Tokenizer
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)
texts = content.loc[:, '热搜话题词']
labels = content.loc[:, 'transformed_label']
# Dataset and DataLoader
dataset = MultiClassDataset(texts, labels, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Initialize Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTWithAttention(PRETRAINED_MODEL_NAME, num_labels=NUM_CLASSES)
model = model.to(device)

# Optimizer and Loss
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss()
loss_set = []
# Training Loop
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad() # reset the gradients to zero before the backward pass.
        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loss = total_loss/len(dataloader)
        loss_set.append(loss)

    print(f"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(dataloader)}")

# Inference Example
model.eval()
test_texts = ["2005年除夕夜的烟花", "冬天哈尔滨像开了魔法滤镜"]
test_encodings = tokenizer(test_texts, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt")
test_input_ids = test_encodings["input_ids"].to(device)
test_attention_mask = test_encodings["attention_mask"].to(device)

with torch.no_grad():
    predictions = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
    predicted_classes = torch.argmax(predictions, dim=1)
    print("Predicted Classes:", predicted_classes.cpu().numpy())

import matplotlib.pyplot as plt
plt.plot(loss_set)
plt.show()