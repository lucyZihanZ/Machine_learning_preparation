from transformers import BertTokenizer, BertModel
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Custom Dataset for Multi-Class Classification
class MultiClassDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]

        encoding = self.tokenizer.encode_plus(
            text,
            max_length=self.max_len,
            truncation=True,
            add_special_tokens=True,
            padding="max_length",
            return_attention_mask=True,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(), # numerical representations of tokens
            "attention_mask": encoding["attention_mask"].squeeze(), 
            "label": torch.tensor(label, dtype=torch.long),
        }

# Model with Custom Attention Mechanism
# input embedding -> add & norm (half) --> multi-head attention -->feedforward(applied nonlinearity and transformation) --> 
# if the problem is translation problems, embedding result will go into multi-head 
# then feedforward and get the output.
class BERTWithAttention(nn.Module):
    def __init__(self, bert_model_name, num_labels):
        super(BERTWithAttention, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name) # multi-head attention within the BertModel
        self.dropout = nn.Dropout(0.5) # dropout: overfitting
        self.fc = nn.Linear(768, num_labels) # fc: fully connected layers + softmax to predict lables.
        self.softmax = nn.Softmax(dim=1) # softmax for multidimensional
        
# The classifier is a linear layer that takes the combined features from the BERT model (the CLS token output and the context vector) and maps them to the output classes for the classification task. 

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.pooler_output  # recognize the whole sentence.

        # Custom attention mechanism
        """
        attention_scores = self.attention(hidden_states)  # Shape: (batch_size, seq_len, head_num)
        attention_weights = torch.softmax(attention_scores, dim=1)  # Normalize scores
        attention_weights = attention_weights.unsqueeze(-1)
        """  
        x = self.dropout(cls_output)
        x = self.fc(x)
        return self.softmax(x)

PRETRAINED_MODEL_NAME = "bert-base-uncased"
MAX_LEN = 128
BATCH_SIZE = 16
EPOCHS = 5
LEARNING_RATE = 2e-7
NUM_CLASSES = 6  # Number of groups

# Example data
import pandas as pd
content = pd.read_csv('/Users/zihanzhao/Documents/Job Application/找工资料/Machine_learning_preparation/Disney.csv', sep=',')
content = content.iloc[:,:-1]
content['transformed_label'] = content['打标类型'].map({
    '品牌热点':0,
    '景色场景': 1,
    '明星热点': 2,
    '生活时事': 3,
    '影视热点': 4,
    '赛事热点': 5
})
content.rename(columns={'打标类型':'type'}, inplace=True)
# 0.8 train and 0.2 test
brands = content.loc[content['transformed_label']==0,'transformed_label'].count()/content.shape[0]
views = content.loc[content["transformed_label"]==1, 'transformed_label'].count()/content.shape[0]
"""
texts = [
    "The study focused on the impact of age and gender on climate policies.",
    "Environmental conservation is critical for all age groups.",
    "Sexual health awareness campaigns are important for younger age groups.",
    "Climate change mitigation policies need funding.",
    "Gender roles in climate policies",
]
labels = [2, 0, 1, 0, 2]  # Example labels
"""
# Tokenizer
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)
bert_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)

texts = content.loc[:, '热搜话题词']
labels = content.loc[:, 'transformed_label']
# Dataset and DataLoader
dataset = MultiClassDataset(texts, labels, tokenizer, MAX_LEN)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Initialize Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BERTWithAttention(PRETRAINED_MODEL_NAME, num_labels=NUM_CLASSES)
model = model.to(device)

# Optimizer and Loss
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss()
loss_set = []
# Training Loop
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)
        optimizer.zero_grad() 
        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        loss = total_loss/len(dataloader)
        loss_set.append(loss)

    print(f"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(dataloader)}")

# Inference Example
model.eval()
test_texts = ["2005年除夕夜的烟花", "冬天哈尔滨像开了魔法滤镜"]
test_encodings = tokenizer(test_texts, truncation=True, padding="max_length", max_length=MAX_LEN, return_tensors="pt")
test_input_ids = test_encodings["input_ids"].to(device)
test_attention_mask = test_encodings["attention_mask"].to(device)

with torch.no_grad():
    predictions = model(input_ids=test_input_ids, attention_mask=test_attention_mask)
    predicted_classes = torch.argmax(predictions, dim=1)
    print("Predicted Classes:", predicted_classes.cpu().numpy())

import matplotlib.pyplot as plt
plt.plot(loss_set)
plt.show()

# compared with the traditional models:
# random forest, 
tokenized_comments = BertTokenizer.from_pretrained('bert-base-uncased')
tokenized_model = BertTokenizer.from_pretrained('bert-base-uncased')