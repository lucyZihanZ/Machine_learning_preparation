# Stat 415 Spring 2025
Stat 415 course materials

# Resources to set up coding environment

[Scikitlearn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html) <br/> [Scikitlearn 2](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) <br/> [Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) <br/> [Matplotlib](https://matplotlib.org/stable/tutorials/index.html) <br/> [Google Colab](https://colab.research.google.com/)  <br/>

# Course Lectures 

Lecture notes can be found on the course Canvas website. 


| Lecture                  |  Date | Material | Readings                
|--------------------------|-------|----------|----------------------------|
| Week 1, Tuesday        | April 1 | Data Cleaning, EDA, Class Imbalance, Imputation                                              | [EDA](https://lewtun.github.io/dslectures/lesson03_data-cleaning/) <br/> [Data Imbalance](https://imbalanced-learn.org/stable/introduction.html) <br/> [All models are wrong](https://www-sop.inria.fr/members/Ian.Jermyn/philosophy/writings/Boxonmaths.pdf)   <br/> [What's wrong with social sciences](https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/) <br/> [Future of Data Analysis](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full) |
| Week 1, Wednesday         | April 2  | Information Theory   <br/> Principle Componenet Analysis (PCA)                                | [Information Theory (2.1-2.5)](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf) <br/> [PCA code](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html) <br/> [PCA Theory](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf) <br/> [Information Theory & Physics](https://bayes.wustl.edu/etj/articles/theory.1.pdf) |
| Week 2. Monday        | April 7 | Clustering, K-Means, K-Means++, DBSCAN, <br/>  Expectation Maximization (EM)                                 | [K-means overview and code](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) <br/> [K-means clustering note](https://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf) <br/> [K-means & EM Code](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html) <br/> [K-means theory](https://proceedings.neurips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf) <br/> [K-means and optimization theory](https://www.claytonthorrez.com/ml-fun/kmeans_gd/index.html) <br/> [DB Scan Wikipedia](https://en.wikipedia.org/wiki/DBSCAN) <br/> [More on silhouette_scores](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) |
| Week 2, Wednesday          | April 9 | Recommender Systems, <br/> Popularity Filtering, <br/> Content-based Filtering, <br/> Collaborative Filtering                   | [Stanford Slides](https://web.stanford.edu/class/cs124/lec/collaborativefiltering21.pdf) <br/> [Textbook Chapter](http://infolab.stanford.edu/~ullman/mmds/ch9.pdf) <br/> [Starter Code for Collab Filtering](https://www.kaggle.com/code/vishorita/best-recommendation-collabarative-filtering?scriptVersionId=119356689) <br/> [Starter for content filtering](https://heartbeat.comet.ml/recommender-systems-with-python-part-i-content-based-filtering-5df4940bd831) <br/> [More content based filtering](https://www.kdnuggets.com/2020/08/content-based-recommendation-system-word-embeddings.html) <br/> [More collaborative filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/) <br/> [Matrix Factorization](https://developers.google.com/machine-learning/recommendation/collaborative/basics) <br/> [Textbook](http://pzs.dstu.dp.ua/DataMining/recom/bibl/1aggarwal_c_c_recommender_systems_the_textbook.pdf)  |
| Week 3, Monday       | April 14 | Recommender Systems Part 2 | |
| Week 3, Wednesday         | April 16 | Linear Regression, Gradient Descent                                                           | [Toronto slides](https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/slides/lec06-slides.pdf) <br/> [Boyd 9.3 (p. 466)](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) <br/> [Gradient Descent](https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/05-grad-descent-scribed.pdf) <br/> [SGD convergence rate](https://www.cs.ubc.ca/~schmidtm/Courses/540-W19/L11.pdf) <br/> [Andrew Ng's notes (p. 1-15)](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) <br/> [Linear Regression code](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_regression_in_sklearn.html) |
| Week 4, Monday           | April 21 |  [**Remote Class on Zoom**]   <br/> Classification, L1 & L2 Regularization, <br/> Cross Validation, F1-Score, <br/> Precision, AIC, BIC  |[Classification in PyTorch](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py) <br/> [Andrew Ng's notes (p. 16-21)](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) <br/> [More Andrew Ng notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes5.pdf) <br/> [Regularization notes](https://web.stanford.edu/class/stats50/files/STATS_50_Regularized_Linear_Regression.pdf) <br/> [Lasso code](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html) <br/> [Regularization Code](https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression) <br/> [Model metrics](https://proclusacademy.com/blog/practical/precision-recall-f1-score-sklearn/) <br/> [L1 vs L2 theory](https://icml.cc/Conferences/2004/proceedings/papers/354.pdf) |
| Week 4, Wednesday      | April 23 | No Class | |
| Week 5, Monday         | April 28 | Random Forests, Decision Trees   | [Decision Trees](https://www.cis.upenn.edu/~danroth/Teaching/CS446-17/LectureNotesNew/dtree/main.pdf) <br/> [Advanced Decision Trees](https://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf) <br/> [Random Forests Textbook](https://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf) <br/> [Random Forests Code](https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/) <br/> [Random Forests simple code](https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/) <br/> [Causal Forests](https://arxiv.org/pdf/1510.04342.pdf) <br/> [Tuning Forests](https://andrewpwheeler.com/2022/10/10/hyperparameter-tuning-for-random-forests/)  |
| Week 5, Wednesday        | April 30 | Deep Learning. Backprop Through Time.| [Textbook Chapter](https://www.deeplearningbook.org/contents/mlp.html) <br/> [Deep Learning Code](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py) <br/> [Andrej Karpathy YouTube Lecture](https://www.youtube.com/watch?v=i94OvYb6noo) <br/> [Backprop](https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/readings/L04%20Backpropagation.pdf) <br/> [MLP Notes](https://www.cs.toronto.edu/~rgrosse/courses/csc311_f21/readings/Multilayer%20Perceptrons.pdf) |
| Week 6, Monday       | May 5   | [**Remote Class on Zoom**]  <br/> Advanced Deep Learning. Activation, Initialization, Advanced Optimizers, Recurrent Neural Nets <br/>   | [Vanishing Gradients](https://proceedings.mlr.press/v28/pascanu13.pdf) <br/> [Dropout](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) <br/> [Xavier Derivation](https://cs230.stanford.edu/section/4/) <br/> [More coding](https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py) <br/> [Optimizers](https://www.ruder.io/optimizing-gradient-descent/) <br/>  [Alex Graves Thesis](https://www.cs.toronto.edu/~graves/phd.pdf) <br/> [Seq2Seq paper](https://arxiv.org/abs/1409.3215) <br/> [CharRNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) <br/> [Momentum and Init](https://proceedings.mlr.press/v28/sutskever13.html) <br/> [Exploding Gradients](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf) |
| Week 6, Wednesday        | May 7| Convolutional Neural Nets. Batch Norm. Max Pooling   | [Code](https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py) <br/> [More code](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py) <br/> [Convolution Tutorial](https://gregorygundersen.com/blog/2017/02/24/cnns/) <br/> [Conv Nets Long tutorial](https://jasoncantarella.com/downloads/CNN.pdf) <br/> [Andrej Karpathy YouTube Lecture](https://www.youtube.com/watch?v=LxfUGhug-iQ) <br/> [ImageNet Paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) <br/> [Stanford Notes](https://cs231n.github.io/convolutional-networks/) <br/> [Conv net for the 2020s](https://arxiv.org/abs/2201.03545) <br/> [Res Nets](https://arxiv.org/abs/1512.03385)  |
| Week 7, Monday       | May 12    | Transfer Learning. Fine Tuning. MAML. Prototypical Nets. <br/> Few shot learning in LLMs                                       |[Prototypical Nets](https://arxiv.org/abs/1703.05175) <br/> [MAML](https://arxiv.org/abs/1703.03400) <br/> [MAML doesn't work](https://arxiv.org/abs/1909.09157) <br/> [Fine tuning code](https://d2l.ai/chapter_computer-vision/fine-tuning.html) <br/> [Clustering With Bregman](https://www.jmlr.org/papers/volume6/banerjee05b/banerjee05b.pdf) |
| Week 7, Wednesday      | May 14  | Black Box Attacks. Interpretability. Integrated Gradients    | [Integrated Gradients](https://arxiv.org/abs/1703.01365) <br/> [SmoothGrad](https://arxiv.org/abs/1706.03825) <br/> [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) <br/> [PDP and ALE](https://christophm.github.io/interpretable-ml-book/pdp.html) <br/> [Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley) <br/> [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) <br/> [Saliency Maps](https://christophm.github.io/interpretable-ml-book/pixel-attribution.html) |
| Week 8, Monday     | May 19  | Unsupervised Learning. Variational Autoencoders. <br/> Maximum Likelihood                            | [VAEs explained](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) <br/> [From VAE to VQVAE](https://lilianweng.github.io/posts/2018-08-12-vae/) <br/> [VAE Code](https://avandekleut.github.io/vae/) <br/> [VAE theory](https://arxiv.org/pdf/1906.02691.pdf) <br/> [Posterior Collapse](https://proceedings.neurips.cc/paper_files/paper/2019/file/7e3315fe390974fcf25e44a9445bd821-Paper.pdf)|
| Week 8, Wednesday  |  May 21  | Stability, Reproducibility, and Algorithmic Fairness                                        | Coming soon  |
| Week 9, Monday    |  May 26 | Holiday, no class  | [Memorial Day](https://en.wikipedia.org/wiki/Memorial_Day)  |
| Week 9, Wednesday    |  May 28 | Everything else you need to know about. <br/> More on time series <br/> SVM, Kernel Methods, Boosting <br/> Pruning. Distillation. Quantization. <br/> Bias-Variance tradeoff <br/> Double Descent | [Kernel Regression](http://faculty.washington.edu/yenchic/18W_425/Lec9_Reg01.pdf) <br/> [Gradient Boosting theory](https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf) <br/> [Gradient Boosting](https://jerryfriedman.su.domains/ftp/stobst.pdf) <br/> [SVM slides](https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf) <br/> [Pruning](https://arxiv.org/abs/1810.02340) <br/> [Distillation & Quantization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/) <br/> [Dobule Descent](https://www.pnas.org/doi/10.1073/pnas.1903070116) <br/> [Double Descent interactive explanation](https://mlu-explain.github.io/double-descent/) <br/> [Time Series Code](https://www.cienciadedatos.net/documentos/py27-time-series-forecasting-python-scikitlearn.html) <br/> [Time Series](https://wandb.ai/iamleonie/A-Gentle-Introduction-to-Time-Series-Analysis-Forecasting/reports/A-Gentle-Introduction-to-Time-Series-Analysis-Forecasting--VmlldzoyNjkxOTMz)|
| Week 10, Monday  |  June 2  | Towards AGI <br/> Transformers, RL, How Chat GPT works. <br/> AI scaling laws  | [Deep RL from human prefrences](https://arxiv.org/abs/1706.03741) <br/> [Introduction to RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) <br/> [More RL](https://wandb.ai/mukilan/intro_to_rl/reports/A-Gentle-Introduction-to-Reinforcement-Learning-With-An-Example--VmlldzoyODc4NzY0) <br/> [GPT](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) <br/> [Illustrated GPT](https://jalammar.github.io/illustrated-gpt2/) <br/>  [Transformers](https://peterbloem.nl/blog/transformers) <br/> [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/01/attention.html) <br/> [Scaling Laws](https://arxiv.org/pdf/2203.15556.pdf) |
| Week 10, Wednesday   |  June 4 | Final exam. <br/> You may start exam at any time on June 4th <br/> and will have three hours to complete it  <br/> from the time you choose to start. No class.    | [Best foods for studying](https://www.healthline.com/nutrition/brain-food-for-studying#The-bottom-line) <br/> [You and Your research](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html) <br/> [Writing papers](https://cs.stanford.edu/people/widom/paper-writing.html) <br/> [Data science interviews](https://github.com/alexeygrigorev/data-science-interviews/blob/master/awesome.md) <br/> [Shannon on creativity](https://jamesclear.com/great-speeches/creative-thinking-by-claude-shannon) <br/> [Best places to visit in the US](https://travel.usnews.com/rankings/best-usa-vacations/)  |




# Homeworks and Due Dates


| Project title                  | Date released | Due date                
|--------------------------------|---------------|-------------------------|
| Fraud Detection                | April 2      | April 16 (2 weeks)  |
| Recommender Systems 1         | April 16      | Apr 28 (1.5 weeks)  |
| Recommender Systems 2         | April 28      | May 7 (1.5 weeks)  |
| Housing Prices     | May 7         | May  19  (1.5 weeks)  |
| Explainability & Animal Images | May 19        | May 28    (1.5 weeks) | 
| Final Exam (Coding)            | June 4       | June 4 (3 hours)    |
